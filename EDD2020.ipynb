{"cells":[{"metadata":{"id":"RjHxX2RdL6zZ","outputId":"c7ac4b7e-d170-4855-a8ae-d571b64574cd","trusted":true},"cell_type":"code","source":"!pip install meditorch\n!pip install torchsummary","execution_count":null,"outputs":[]},{"metadata":{"id":"KLbWwuezxQI9"},"cell_type":"markdown","source":"# **EDD**"},{"metadata":{"id":"e_fRixUtyEQF"},"cell_type":"markdown","source":"**1.Downloading data and extracting files to ./EDD2020**"},{"metadata":{"id":"MoNTIUi-_sxD","outputId":"20b56c53-833c-49f5-b185-8fa3935bc9d5","trusted":true},"cell_type":"code","source":"#GET THE DATA SET AFTER ACCEPTING RULES AT https://edd2020.grand-challenge.org/\nimport zipfile\nwith zipfile.ZipFile('EDD2020.zip', 'r') as zip_ref:\n    \n    zip_ref.extractall('EDD2020')\n!ls ./EDD2020/EDD2020_release-I_2020-01-15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimg = cv2.imread('./EDD2020/EDD2020_release-I_2020-01-15/originalImages/EDD2020_ACB0000.jpg')\nimg = img[:,:,::-1]\n#img = img[::-1,:,:]\nplt.imshow(img)\nplt.show()\nnp.asarray(img).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize_my_images(src,dst,is_masks):\n    import cv2\n    import os\n    import numpy as np\n\n    i = 1\n    img_size = 224\n    path = src\n    for img_name in sorted(os.listdir(path)):\n        img = None\n        print(img_name)\n        try:\n            if not is_masks:\n                img = cv2.imread(os.path.join(path, img_name))\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            elif is_masks:\n                img = cv2.imread(os.path.join(path, img_name),cv2.IMREAD_GRAYSCALE)\n                \n            h, w = img.shape[:2]\n            a1 = w/h\n            a2 = h/w\n\n            if(a1 > a2):\n                #print('a1 > a2')\n                # if width greater than height\n                r_img = cv2.resize(img, (round(img_size * a1), img_size), interpolation = cv2.INTER_AREA)\n                margin = int(r_img.shape[1]/6)\n                crop_img = r_img[0:img_size, margin:(margin+img_size)]\n\n            elif(a1 < a2):\n                #print('a1 < a2')\n                # if height greater than width\n                r_img = cv2.resize(img, (img_size, round(img_size * a2)), interpolation = cv2.INTER_AREA)\n                margin = int(r_img.shape[0]/6)\n                crop_img = r_img[margin:(margin+img_size), 0:img_size]\n\n            elif(a1 == a2):\n                print('a1== a2')\n                # if height and width are equal\n                r_img = cv2.resize(img, (img_size, round(img_size * a2)), interpolation = cv2.INTER_AREA)\n                crop_img = r_img[0:img_size, 0:img_size]\n\n            if(crop_img.shape[0] != img_size or crop_img.shape[1] != img_size):\n                #print('someting....')\n                crop_img = r_img[0:img_size, 0:img_size]\n\n            if(crop_img.shape[0] == img_size and crop_img.shape[1] == img_size):\n\n                print(\"Saving image with dims: \" + str(crop_img.shape[0]) + \"x\" + str(crop_img.shape[1]))\n                if not is_masks:\n                    cv2.imwrite(dst + img_name, crop_img[:,:,::-1])#SAVING AS RGB FORMAT \n                elif is_masks:\n                    cv2.imwrite(dst + img_name, crop_img)\n                i += 1\n            #print('><'*20)\n        except:\n            print('Could not save image.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir ./EDD2020/resized_masks/\nresize_my_images('./EDD2020/EDD2020_release-I_2020-01-15/masks/','./EDD2020/resized_masks/',is_masks=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir ./EDD2020/resized_images/\nresize_my_images('./EDD2020/EDD2020_release-I_2020-01-15/originalImages/','./EDD2020/resized_images/',is_masks=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"AC4VNDt4vggK"},"cell_type":"markdown","source":"**2.Defining class for preparing the dataset and other utility functions**\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n#img= Image.open('./EDD2020/resized_images/EDD2020_ACB0000.jpg').convert('RGB')\ndef display_image(img):\n    from matplotlib import pyplot as plt\n    %matplotlib inline\n    plt.imshow(img)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"O7O_z0kiyAST","trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\nimport torch\nimport os\nimport numpy as np\nimport glob\nfrom PIL import Image\nfrom skimage.transform import resize\n\ndef load_image(path,is_mask):\n    if not is_mask:\n        return np.asarray(Image.open(path).convert(\"RGB\"))\n    else:\n        return np.asarray(Image.open(path).convert('L'))\n\ndef load_set(folder,is_mask,shuffle=False):\n    data = []\n    img_list = sorted(glob.glob(os.path.join(folder, '*.png')) + \\\n                      glob.glob(os.path.join(folder, '*.jpg')) + \\\n                      glob.glob(os.path.join(folder, '*.tif')) + \\\n                      glob.glob(os.path.join(folder, '*.jpeg')))\n    if shuffle:\n        np.random.shuffle(img_list)\n    for img_fn in img_list:\n        img = load_image(img_fn,is_mask)\n        data.append(img)\n    return data, img_list\n\nclass EDD(Dataset):\n    '''\n    Class for preparing the EDD2020 dataset\n    '''\n    def __init__(self, root, transform=None):\n        self.root = root\n        self.transform = transform\n        self.original_images = None\n        self.masks = None\n        self.labels = None\n        self._extract_images_and_segments(root)\n\n    def __getitem__(self,index):\n        img = self.original_images[index]\n        mask = self.masks[index]\n        label = self.labels[index]\n        label = torch.as_tensor(label, dtype=torch.int32)\n        \n#         boxes = []\n#         try:\n#             for i in range(len(mask)):\n#                 #mask[i] = np.asarray(mask[i])\n#                 pos  = np.where(mask[i])\n#                 xmin = np.min(pos[1])\n#                 xmax = np.max(pos[1])\n#                 ymin = np.min(pos[0])\n#                 ymax = np.max(pos[0])\n#                 boxes.append([xmin, ymin, xmax, ymax])\n#         except ValueError:\n#             boxes.append([0, 0, 0, 0])\n\n#         boxes = torch.as_tensor(boxes, dtype=torch.float32)\n#         mask = torch.as_tensor(mask, dtype=torch.uint8)\n#         image_id = torch.tensor([index])\n        \n        if self.transform:\n            img = self.transform(img)\n        else:\n            transform_to_tensor = transforms.Compose([transforms.ToTensor(),])\n            img = transform_to_tensor(img)\n            \n            \n#         target = {}\n#         target[\"boxes\"] = boxes\n#         target[\"labels\"] = label\n#         target[\"masks\"] = mask\n#         target[\"image_id\"] = image_id\n        \n        return img, mask\n\n    def __len__(self):\n        return len(self.original_images)\n    \n    def _extract_images_and_segments(self,global_path):\n        '''\n        Function to process images and their respective masks.\n        It sets  self.original_images and self.masks to processed images at the end.\n        '''\n        images_path = os.path.join(global_path, 'resized_images')\n        all_images, img_filenames = load_set(folder=images_path,is_mask=False)\n        img_filenames_with_ext = [os.path.split(fn)[-1] for fn in img_filenames]\n        img_filenames_wo_ext = [fn[:fn.rfind('.')] for fn in img_filenames_with_ext]\n\n        classes = ['BE','suspicious','HGD','cancer','polyp']\n\n        masks_path = os.path.join(global_path, 'resized_masks')\n        all_masks, mask_filenames = load_set(folder=masks_path,is_mask=True)\n        mask_filenames_with_ext = [os.path.split(fn)[-1] for fn in mask_filenames]\n        mask_filenames_wo_ext = [fn[:fn.rfind('.')] for fn in mask_filenames_with_ext]\n        temp_dict={}#contains 502 mask filenames as keys and respective masks as values\n        for i in range(len(all_masks)):\n            temp_dict[mask_filenames_wo_ext[i]]=all_masks[i]\n\n        all_masks=[]\n        all_labels=[]\n        for img in img_filenames_wo_ext:\n            masks_for_img = []\n            temp_labels = []\n            for c in classes:\n                try:\n                    mask_file_name = img+'_'+c\n                    temp_dict[mask_file_name] = np.where(temp_dict[mask_file_name] > 0, 1, 0)\n                    temp_dict[mask_file_name] = temp_dict[mask_file_name].astype(np.float32)\n                    masks_for_img.append(temp_dict[mask_file_name].reshape(temp_dict[mask_file_name].shape+ (1,)))\n                    temp_labels.append(1)\n                except KeyError:\n                    dummy = np.zeros((224, 224)).astype(np.float32)\n                    masks_for_img.append(dummy.reshape(dummy.shape + (1,)))\n                    temp_labels.append(0)\n            temp = None\n            temp = np.concatenate(masks_for_img,2)#temp.shape     (224, 224, 5)\n            temp = temp.reshape((1,)+temp.shape)#temp.shape (1, 224, 224, 5)\n            all_masks.append(temp)\n            all_labels.append(temp_labels)\n            \n            \n        all_masks = np.vstack(all_masks)#all_masks.shape (386, 224, 224, 5)\n        all_masks = np.moveaxis(all_masks,source=3,destination=1)#all_masks.shape (386, 5, 224, 224)\n        \n        all_images = np.asarray(all_images)\n        all_images = all_images.astype(np.uint8)\n        \n        print('len(all_images):',len(all_images),'len(all_masks):',len(all_masks),' len(all_labels):',len(all_labels))\n        \n        print('>>>>>>>>>>>Images<<<<<<<<<<<')\n        print('type(all_images):',type(all_images),' all_images.shape:',all_images.shape)\n        print('type(all_images[1]):',type(all_images[1]),' all_images[1].shape:',all_images[1].shape)\n        print('.'*100)\n        print('>>>>>>>>>>>Masks<<<<<<<<<<<<')\n        print('type(all_masks):',type(all_masks),'all_masks.shape:',all_masks.shape)\n        print('type(all_masks[1]):',type(all_masks[1]),'all_masks[1].shape:',all_masks[1].shape)\n        print('.'*100)\n        \n        self.masks = all_masks\n        self.original_images = all_images\n        self.labels = all_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>SCRATCH PAD<<<<<<<<<<<<<<<<<<<<<<<<<\n# from torch.utils.data import DataLoader\n# from torch.utils.data.sampler import SubsetRandomSampler\n# np.random.seed(42)\n\n\n# dataset = EDD('./EDD2020/')\n# # for i in range(25):\n# #     print(i+1,':->',type(dataset[i][0]),dataset[i][0].shape,type(dataset[i][1]),dataset[i][1].shape)\n# dataset_size = 100#len(dataset)\n# indices = list(range(dataset_size))\n# split = int(np.floor(0.25 * dataset_size))\n# #np.random.shuffle(indices)\n# train_indices, val_indices = indices[split:], indices[:split]\n# train_sampler = torch.utils.data.sampler.SequentialSampler(train_indices)#change to SubsetRandomSampler\n# loader = DataLoader(dataset, batch_size=4, sampler=train_sampler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for batch in loader:\n#     print(len(batch))\n#     print(batch[0].shape,batch[1].shape)\n#     break\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Y41kbTZXvuai"},"cell_type":"markdown","source":"**3.instantiating class and splitting it into train,val set**\n"},{"metadata":{"id":"JEsu1HcnmlNA","outputId":"9ae8c54d-4e27-46dd-9657-0d0c133d1618","trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nnp.random.seed(42)\ndef get_edd_loader(path,validation_split=.25,shuffle_dataset=True):\n    dataset = EDD(path)#instantiating the data set.\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    split = int(np.floor(validation_split * dataset_size))\n    if shuffle_dataset :\n        np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n    train_sampler = SubsetRandomSampler(train_indices)\n    valid_sampler = SubsetRandomSampler(val_indices)\n    loader={\n        'train':DataLoader(dataset, batch_size=2, sampler=train_sampler),\n        'val':DataLoader(dataset, batch_size = 2,sampler=valid_sampler)\n    }\n    return loader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.Defining Trainer and losses**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# from collections import defaultdict\n# import copy\n# import time\n# import torch\n# from meditorch.utils.plot import metrics_line, normalise_mask\n# import numpy as np\n\n# def dice_loss(pred, target, smooth = 1.):\n#     pred = pred.contiguous()\n#     target = target.contiguous()\n\n#     intersection = (pred * target).sum(dim=2).sum(dim=2)\n\n#     loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n\n#     return loss.mean()\n\n# def intersection_over_union(target, pred):\n\n#     target = target.flatten()\n#     pred = pred.flatten()\n\n#     intersect = np.dot(target, pred)\n#     union = (target + pred).sum() - intersect\n#     return intersect/union\n\n# def calc_loss(pred, target, metrics, bce_weight=0.5):\n#     bce = torch.nn.functional.binary_cross_entropy_with_logits(pred, target)\n#     pred = torch.sigmoid(pred)\n#     dice = dice_loss(pred, target)\n\n#     pred_binary = normalise_mask(pred.detach().cpu().numpy())\n#     iou = intersection_over_union(target.detach().cpu().numpy(), pred_binary)\n\n#     loss = bce * bce_weight + dice * (1 - bce_weight)\n\n#     metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n#     metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n#     metrics['iou'] += iou * target.size(0)\n#     metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n\n#     return loss\n\n# def compute_metrics(metrics, epoch_samples):\n#     computed_metrics = {}\n#     for k in metrics.keys():\n#         computed_metrics[k] = metrics[k] / epoch_samples\n#     return computed_metrics\n\n# def print_metrics(computed_metrics, phase):\n#     outputs = []\n#     for k in computed_metrics.keys():\n#         outputs.append(\"{}:{:4f}\".format(k, computed_metrics[k]))\n#     print(\"\\t{}-> {}\".format(phase.ljust(5), \"|\".join(outputs)))\n\n# class Trainer(object):\n\n#     def __init__(self, model, optimizer=None, scheduler=None):\n\n#         super().__init__()\n#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#         print(\"Customized trainer\\t\",'Device====>',self.device)\n#         self.model = model.to(self.device)\n#         self.optimizer = optimizer\n#         if self.optimizer == None:\n#             self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n\n#         self.scheduler = scheduler\n#         if self.scheduler == None:\n#             self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)\n\n\n#     def train_model(self, dataloaders, num_epochs=25):\n#         best_model_wts = copy.deepcopy(self.model.state_dict())\n#         best_loss = 1e10\n#         epochs_metrics = {\n#             'train': [],\n#             'val': []\n#         }\n\n#         for epoch in range(num_epochs):\n#             print('Epoch {}/{}:'.format(epoch+1, num_epochs))\n\n#             since = time.time()\n\n#             # Each epoch has a training and validation phase\n#             for phase in ['train', 'val']:\n#                 if phase == 'train':\n#                     for param_group in self.optimizer.param_groups:\n#                         print(\"\\tlearning rate: {:.2e}\".format(param_group['lr']))\n#                     self.model.train()  # Set model to training mode\n#                 else:\n#                     self.model.eval()   # Set model to evaluate mode\n#                 metrics = defaultdict(float)\n#                 epoch_samples = 0\n\n#                 for inputs, targets in dataloaders[phase]:#phase is either train or val\n#                     masks = targets\n#                     #print('>>>>>>>>>>>',inputs.shape,masks.shape,'<<<<<<<<<<<<<<<<<<')\n                \n\n#                     inputs = inputs.to(self.device)\n#                     masks = masks.to(self.device)\n\n#                     # zero the parameter gradients\n#                     self.optimizer.zero_grad()\n\n#                     # forwar track history for training phase only\n#                     with torch.set_grad_enabled(phase == 'train'):\n#                         outputs = self.model(inputs)\n#                         loss = calc_loss(outputs, masks, metrics)\n\n#                         # backward + optimize only if in training phase\n#                         if phase == 'train':\n#                             loss.backward()\n#                             self.optimizer.step()\n\n#                     # statistics\n#                     epoch_samples += inputs.size(0)\n\n#                 computed_metrics = compute_metrics(metrics, epoch_samples)\n#                 print_metrics(computed_metrics, phase)\n#                 epochs_metrics[phase].append(computed_metrics)\n#                 epoch_loss = metrics['loss'] / epoch_samples\n\n#                 if phase == 'train':\n#                     self.scheduler.step()\n\n#                 # deep copy the model\n#                 if phase == 'val' and epoch_loss < best_loss:\n#                     print(\"\\tCurrent epoch loss {:4f} is less than previous epoch loss {:4f}\".format(epoch_loss, best_loss))\n#                     best_loss = epoch_loss\n#                     best_model_wts = copy.deepcopy(self.model.state_dict())\n\n#             time_elapsed = time.time() - since\n#             print('\\t{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n#             print('-' * 10)\n\n#         print('Best val loss: {:4f}'.format(best_loss))\n#         print('Saving best model')\n#         self.model.load_state_dict(best_model_wts)# load best model weights\n\n#         metrics_line(epochs_metrics)\n\n#     def predict(self, X, threshold=0.5):\n#         self.model.eval()\n#         inputs = X.to(self.device)\n#         pred = self.model(inputs)\n#         pred = pred.data.cpu().numpy()\n#         pred = normalise_mask(pred, threshold)\n#         return pred\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5.Defining the model Unet+Resnet from meditorch**  \nreference: https://github.com/jinglescode/meditorch"},{"metadata":{"id":"DuQTnW__nrUr","trusted":true},"cell_type":"code","source":"from meditorch.nn.models import UNetResNet\nfrom torchsummary import summary\nimport torch.optim as optim\nfrom meditorch.nn import Trainer\n\nfrom torch.optim import lr_scheduler\nfrom meditorch.utils.plot import plot_image_truemask_predictedmask\n\nmodel = UNetResNet(in_channel=3, n_classes=5)\noptimizer_func = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer_func, step_size=10, gamma=0.1)\ntrainer = Trainer(model, optimizer=optimizer_func, scheduler=scheduler)","execution_count":null,"outputs":[]},{"metadata":{"id":"gcTb5_suwQ4M"},"cell_type":"markdown","source":"**6.Training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"loader = get_edd_loader('./EDD2020/',validation_split=.20,shuffle_dataset=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.train_model(loader, num_epochs=2)","execution_count":null,"outputs":[]},{"metadata":{"id":"RRuSTEddwY-m"},"cell_type":"markdown","source":"**7.Prediction**"},{"metadata":{"id":"MXarNgrKoA64","trusted":true},"cell_type":"code","source":"images, target = next(iter(loader['val']))#fetch the next batch of images and respective masks","execution_count":null,"outputs":[]},{"metadata":{"id":"DqRYdi6boOBd","trusted":true},"cell_type":"code","source":"preds = trainer.predict(images)\nprint(images.shape,target.shape,preds.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"uFCWmFFYoPUL","outputId":"b5d3f4c0-bb7c-469f-e739-d70cd1abfab5","trusted":true},"cell_type":"code","source":"plot_image_truemask_predictedmask2(images, target, preds)","execution_count":null,"outputs":[]},{"metadata":{"id":"Tabz4ks0tZUQ","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom functools import reduce\n\n\ndef metrics_line(data):\n    phases = list(data.keys())\n    metrics = list(data[phases[0]][0].keys())\n\n    i = 0\n    fig, axs = plt.subplots(1, len(metrics))\n    fig.set_figheight(4)\n    fig.set_figwidth(4*len(metrics))\n    for metric in metrics:\n        for phase in phases:\n            axs[i].plot([i[metric] for i in data[phase]], label=phase)\n        axs[i].set_title(metric)\n        i+=1\n\n    plt.legend()\n    plt.show()\n\n\ndef normalise_mask(mask, threshold=0.5):\n    mask[mask > threshold] = 1\n    mask[mask <= threshold] = 0\n    return mask\n\ndef reverse_transform(inp):\n    inp = inp.numpy().transpose((1, 2, 0))\n    inp = np.clip(inp, 0, 1)\n    inp = (inp * 255).astype(np.uint8)\n    return inp\n\ndef plot_img_array(img_array, ncol=3):\n    nrow = len(img_array) // ncol\n\n    f, plots = plt.subplots(nrow, ncol, sharex='all', sharey='all', figsize=(ncol * 4, nrow * 4))\n    for i in range(len(img_array)):\n        plots[i // ncol, i % ncol]\n        plots[i // ncol, i % ncol].imshow(img_array[i])\n\ndef plot_image_truemask_predictedmask2(images, labels, preds):\n\n    input_images_rgb = [reverse_transform(x) for x in images]\n    target_masks_rgb = [masks_to_coloredmasks(x) for x in labels]\n    pred_masks_rgb   = [masks_to_coloredmasks(x) for x in preds]\n\n    img_arrays = [input_images_rgb, target_masks_rgb, pred_masks_rgb]\n    flatten_list = reduce(lambda x,y: x+y, zip(*img_arrays))\n    plot_img_array(np.array(flatten_list), ncol=len(img_arrays))\n\ndef apply_mask_color(mask, mask_color):\n    colored_mask = np.concatenate(([mask[ ... , np.newaxis] * color for color in mask_color]), axis=2)\n    return colored_mask.astype(np.uint8)\n\ndef masks_to_coloredmasks(mask, normalise=True, colors=None):\n    '''\n    To assign colors to the different classes, for identification purpose only!\n    '''\n    segments_colors = np.asarray([(0, 255, 255), (0,0,255), (0, 255, 0), (255, 0, 0),(0, 0, 0)])\n    if colors is not None:\n        segments_colors = colors\n\n    if normalise:\n        normalise_mask(mask)\n\n    mask_colored = np.concatenate( [ [apply_mask_color(mask[i], segments_colors[i])] for i in range(len(mask)) ] )\n    mask_colored = np.max(mask_colored, axis=0)\n\n    mask_colored = np.where(mask_colored.any(-1,keepdims=True),mask_colored,255)\n\n    return mask_colored\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"7NfMS48UdYz4","outputId":"31f805a5-8499-44f9-ead2-9652b4dd2542","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for batch_image,batch_mask in (loader['train']): #every iteration fetches data of size batchsize\n    print('adfaf')\n    print(batch_image.shape,batch_mask.shape)\n    for image in batch_image:\n        im = transforms.ToPILImage()(image).convert(\"RGB\")\n        display(im)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dataset[0][0]),dataset[0][0].shape,type(dataset[0][1]),dataset[0][1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO GENERATE MASKS\nt=dataset[5][1]\nt=t.astype(np.float32)\nfor i in range(5):\n    tx=t[i,:,:]\n    tx=tx.reshape((1,)+tx.shape)\n    tx=torch.from_numpy(tx)\n    im=transforms.ToPILImage()(tx)\n    display(im)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t=dataset[0][0]\nfrom torchvision import transforms\nim = transforms.ToPILImage()(t).convert(\"RGB\")\ndisplay(im)\nprint(im.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO COMBINE ALL MULTIPLE MASKS INTO ONE\n    \nif __name__ == '__main__':\n    import tifffile as tiff\n    from tifffile import imsave\n    import cv2\n    import numpy as np\n    import glob\n    import os\n    imgdir='./EDD2020/EDD2020_release-I_2020-01-15/originalImages/'\n    maskdir='./EDD2020/EDD2020_release-I_2020-01-15/masks/'\n    outputdir='./EDD2020/EDD2020_release-I_2020-01-15/outputdir/'\n    \n    os.makedirs(outputdir, exist_ok=True)\n    \n    categoryList = ['BE', 'suspicious', 'HGD' , 'cancer', 'polyp']\n    \n    ext = ['*.jpg']\n    for filename in sorted(glob.glob(imgdir + '/'+ ext[0], recursive = True)):\n        file=filename.split('/')[-1]\n        fileNameOnly = file.split('.')[0]\n        nClasses_annotated = len(glob.glob1(maskdir,fileNameOnly+\"*.tif\"))\n        fileList = glob.glob1(maskdir,fileNameOnly+\"*.tif\")\n       \n        # read original and make stack of mask images\n        image = cv2.imread(filename)\n        height, width = image.shape[:2]\n        mask = np.zeros([height, width, len(categoryList)], dtype=np.uint8)\n        \n        for i in range (0, nClasses_annotated):\n            f = fileList[i].split('_')[-1]\n            if f.split('.')[0] == 'BE':\n                mask[:,:, 0] = tiff.imread(os.path.join(maskdir,fileList[i]))\n            elif (f.split('.')[0]== 'suspicious'):\n                mask[:,:, 1] = tiff.imread(os.path.join(maskdir,fileList[i]))\n            elif (f.split('.')[0] == 'HGD'):\n                mask[:,:, 2] = tiff.imread(os.path.join(maskdir,fileList[i]))\n            elif (f.split('.')[0] == 'cancer'):\n                mask[:,:, 3] = tiff.imread(os.path.join(maskdir,fileList[i]))\n            elif (f.split('.')[0] == 'polyp'):\n                mask[:,:, 4] = tiff.imread(os.path.join(maskdir,fileList[i]))\n                \n                \n            im_mask = mask.transpose([2,0,1])    \n            imsave(os.path.join(outputdir,fileNameOnly+'_mask.tif'), im_mask)      \n            ","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"EDD2020.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}