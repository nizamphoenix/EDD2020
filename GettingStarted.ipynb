{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Getting Started with Clara Train SDK\n",
    "Clara Train SDK consists of different modules as depicted below \n",
    "<br>![side_bar](screenShots/TrainBlock.png)\n",
    "\n",
    "By the end of this notebook you will:\n",
    "1. Understand the components of [Medical Model ARchive (MMAR)](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/mmar.html)\n",
    "2. Know how to configure train config json to train a CNN\n",
    "3. Train a CNN with single and muiltple GPUs\n",
    "4. Fine tune a model\n",
    "5. Export a model \n",
    "6. Perform inference on test dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Prerequisites\n",
    "- Nvidia GPU with 8GB of memory (Pascal or newer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Resources\n",
    "It maybe helpful to watch the free GTC Digital 2020 talk covering the Clara Train SDK \n",
    "- [S22563](https://developer.nvidia.com/gtc/2020/video/S22563)\n",
    "Clara train Getting started: Core concepts, Bring Your Own Components (BYOC), AI assisted annotation (AIAA), AutoML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## DataSet \n",
    "This notebook uses a sample dataset (ie. a single image of spleen dataset) provided in the package to train a network for a few epochs. \n",
    "This single file is duplicated 32 times for the training set and 9 times for validation in order to mimic the full spleen dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "#### Disclaimer  \n",
    "In this Notebook we run sample training jobs for one or two epochs just to highlight the core concepts. \n",
    "A relatively small neural network is also used to ensure it runs on most GPUs.    \n",
    "For realistic training a user could increase the number of epochs, use larger neural networks and tune other parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Lets get started\n",
    "It is helpful to first check that we have an NVIDIA GPU available in the docker by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# following command should show all gpus available \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "The cell below defines a helper function that will be used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MMAR_ROOT=\"/claraDevDay/MMARs/GettingStarted/\"\n",
    "print (\"setting MMAR_ROOT=\",MMAR_ROOT)\n",
    "%ls $MMAR_ROOT\n",
    "\n",
    "!chmod 777 $MMAR_ROOT/commands/*\n",
    "def printFile(filePath,lnSt,lnOffset):\n",
    "    print (\"showing \",str(lnOffset),\" lines from file \",filePath, \"starting at line\",str(lnSt))\n",
    "    lnOffset=lnSt+lnOffset\n",
    "    !< $filePath head -n \"$lnOffset\" | tail -n +\"$lnSt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Medical Model ARchive (MMAR)\n",
    "Clara Train SDK uses the [Medical Model ARchive (MMAR)](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/mmar.html). \n",
    "The MMAR defines a standard structure for organizing all artifacts produced during the model development life cycle. \n",
    "The Clara Train SDK basic idea is to get started on training deep learning models using intuitive configuration files as shown below:\n",
    "<br>![side_bar](screenShots/MMAR.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "You can download sample models for different problems from [NGC](https://ngc.nvidia.com/catalog/models?orderBy=modifiedDESC&pageNumber=0&query=clara&quickFilter=&filters=) <br> \n",
    "All MMAR follow the structure provided in this Notebook. if you navigate to the parent folder structure it should contain the following subdirectories\n",
    "```\n",
    "./GettingStarted \n",
    "├── commands\n",
    "├── config\n",
    "├── docs\n",
    "├── eval\n",
    "├── models\n",
    "└── resources\n",
    "```\n",
    "\n",
    "* `commands` contains a number of ready-to-run scripts for:\n",
    "    - training\n",
    "    - training with multiple GPUS\n",
    "    - validation\n",
    "    - inference (testing)\n",
    "    - exporting models in TensorRT Inference Server format\n",
    "* `config` contains configuration files (in JSON format) for each training, \n",
    "validation, and deployment for [AI-assisted annotation](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/aiaa/index.html) \n",
    "(_Note:_ these configuration files are used in the scripts under the `commands` folder)\n",
    "* `docs` contains local documentation for the model, but for a more complete view it is recommended that you visit the NGC model page\n",
    "* `eval` is used as the output directory for model evaluation (by default)\n",
    "* `models` is where the tensorflow checkpoint-formatted model is stored (`.index`, `.meta`, `.data-xxxxx-of-xxxxx`), and the corresponding graph definition files (`fzn.pb` for frozen models, and `trt.pb` for TRT models)\n",
    "* `resources` currently contains the logger configuration in the `log.config` file\n",
    "\n",
    "Some of the most important files you will need to understand to configure and use in Clara Train SDK are:\n",
    "\n",
    "1. `environment.json` which has important common parameters: \n",
    "    * `DATA_ROOT` is the root folder where the data with which we would like to train, validate, or test resides in\n",
    "    * `DATASET_JSON` expects the path to a JSON-formatted file \n",
    "    * `MMAR_CKPT_DIR` the path to the where the tensorflow checkpoint files reside\n",
    "    * `MMAR_EVAL_OUTPUT_PATH` the path to output evaluation metrics for the neural network during training, validation, and inference\n",
    "    * `PROCESSING_TASK` the type of processing task the neural net is intended to perform (currently limited to `annotation`, `segmentation`, `classification`)\n",
    "    * `PRETRAIN_WEIGHTS_FILE` (_optional_) \tdetermines the location of the pre-trained weights file; if the file does not exist and is needed, \n",
    "    the training program will download it from a predefined URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "printFile(MMAR_ROOT+\"/config/environment.json\",0,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "2. `train.sh` and `train_finetune.sh` run the commands to train the neural network based on the `config_train.json` configuration; \n",
    "this shell script can be also used to override parameters in `config_train.json` using the `--set` argument (see `train_finetune.sh`)\n",
    "\n",
    "_Note_: The main difference between the two is that `train_finetune.sh` specifies a `ckpt` file, \n",
    "while `train.sh` does not since it is training from scratch.\n",
    "\n",
    "Let's take a look at `train.sh` by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "printFile(MMAR_ROOT+\"/commands/train_W_Config.sh\",30,30)\n",
    "printFile(MMAR_ROOT+\"/commands/train_finetune.sh\",0,30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## config.json Main Concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "`config_train.json` contains all the parameters necessary to define the neural network, \n",
    "how is it trained (training hyper-parameters, loss, etc.), \n",
    "pre- and post-transformation functions necessary to modify and/or augment the data before input to the neural net, etc. \n",
    "The complete documentation on the training configuration is laid out \n",
    "[here](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/appendix/configuration.html#training-configuration).\n",
    "The configuration file defines all training related parameters. \n",
    "This is were a researcher would spend most of their time.\n",
    "\n",
    "<br>![s](screenShots/MMARParts.png)<br> \n",
    "\n",
    "Lets take some time to examine each component of this configuration file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Global configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "confFile=MMAR_ROOT+\"/config/trn_base.json\"\n",
    "printFile(confFile,0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "2. Training config which includes:\n",
    "    1. Loss functions:\n",
    "    [Dice](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=dice#module-ai4med.components.losses.dice)\n",
    "    , [CrossEntropy](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=crossentropy#ai4med.components.losses.cross_entropy.CrossEntropy)\n",
    "    , [Focal](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=focal#module-ai4med.components.losses.focal)\n",
    "    , [FocalDice](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=focaldice#ai4med.components.losses.focal_dice.FocalDice) \n",
    "    , [CrossEntropyDice](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=crossentropydice#ai4med.components.losses.cross_entropy_dice.CrossEntropyDice) \n",
    "    , [BinaryClassificationLoss](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=binaryclassificationloss#ai4med.components.losses.classification_loss.BinaryClassificationLoss)\n",
    "    , [MulticlassClassificationLoss](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=multiclassclassificationloss#ai4med.components.losses.classification_loss.MulticlassClassificationLoss)\n",
    "    , [WeightedMulticlassClassificationLoss](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=weightedmulticlassclassificationloss#ai4med.components.losses.classification_loss.WeightedMulticlassClassificationLoss)\n",
    "    2. Optimizer\n",
    "    [Momentum]()\n",
    "    , [Adam]()\n",
    "    , [NovaGrad]()\n",
    "    3. Network architecture\n",
    "    [SegAhnet]()\n",
    "    , [SegResnet]()\n",
    "    , [Unet]()\n",
    "    , [UnetParallel]()\n",
    "    , [DenseNet121]()\n",
    "    , [Alexnet]()\n",
    "    4. Learing rate Policy \n",
    "    [ReducePoly]()\n",
    "    , [DecayOnStep]()\n",
    "    , [ReduceCosine]()\n",
    "    , [ReduceOnPlateau]()\n",
    "    5. Image pipeline\n",
    "        1. Classification \n",
    "        , [ClassificationImagePipeline]()\n",
    "        , [ClassificationImagePipelineWithCache]()\n",
    "        , [ClassificationKerasImagePipeline]()\n",
    "        , [ClassificationKerasImagePipelineWithCache]()\n",
    "        2. Segmenatation \n",
    "        , [SegmentationImagePipeline]()\n",
    "        , [SegmentationImagePipelineWithCache]()\n",
    "        , [SegmentationKerasImagePipeline]()\n",
    "        , [SegmentationKerasImagePipelineWithCache]()    \n",
    "    4. Pretransforms\n",
    "        1. Loading transformations:\n",
    "            [LoadNifti](https://docs.nvidia.com/clara/tlt-m[i/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadnifti#ai4med.components.transforms.load_nifti.LoadNifti)\n",
    "            , [LoadPng](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadpng#ai4med.components.transforms.load_png.LoadPng)\n",
    "            , [ConvertToChannelsFirst](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=converttochannelsfirst#ai4med.components.transforms.convert_to_channels_first.ConvertToChannelsFirst)\n",
    "            , [LoadImageMasksFromNumpy](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadimagemasksfromnumpy#ai4med.components.transforms.load_image_masks_from_numpy.LoadImageMasksFromNumpy)\n",
    "            , [LoadJpg](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadjpg#ai4med.components.transforms.load_jpg.LoadJpg)\n",
    "        2. Resample Transformation\n",
    "            [RepeatChannel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=repeatchannel#ai4med.components.transforms.repeat_channel.RepeatChannel)\n",
    "            , [ScaleByFactor](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scalebyfactor#ai4med.components.transforms.scale_by_factor.ScaleByFactor)\n",
    "            , [ScaleByResolution](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scalebyresolution#ai4med.components.transforms.scale_by_resolution.ScaleByResolution)\n",
    "            , [ScaleBySpacing](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scalebyspacing#ai4med.components.transforms.scale_by_spacing.ScaleBySpacing)\n",
    "            , [ScaleToShape](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scaletoshape#ai4med.components.transforms.scale_to_shape.ScaleToShape)\n",
    "            , [RestoreOriginalShape](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=restoreoriginalshape#ai4med.components.transforms.restore_original_shape.RestoreOriginalShape)\n",
    "            , [LoadImageMasksFromNumpy](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadimagemasksfromnumpy#ai4med.components.transforms.load_image_masks_from_numpy.LoadImageMasksFromNumpy)\n",
    "        3. Cropping transformations\n",
    "            [CropForegroundObject](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropforegroundobject#ai4med.components.transforms.crop_foreground_object.CropForegroundObject)\n",
    "            , [FastPosNegRatioCropROI](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=fastposnegratiocroproi#ai4med.components.transforms.fast_pos_neg_ratio_crop_roi.FastPosNegRatioCropROI)\n",
    "            , [CropByPosNegRatio](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropbyposnegratio#ai4med.components.transforms.crop_by_pos_neg_ratio.CropByPosNegRatio)\n",
    "            , [SymmetricPadderDiv](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=symmetricpadderdiv#ai4med.components.transforms.symmetric_padder_div.SymmetricPadderDiv)\n",
    "            , [FastCropByPosNegRatio](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=fastcropbyposnegratio#ai4med.components.transforms.fast_crop_by_pos_neg_ratio.FastCropByPosNegRatio)\n",
    "            , [CropByPosNegRatioLabelOnly](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropbyposnegratiolabelonly#ai4med.components.transforms.crop_by_pos_neg_ratio_label_only.CropByPosNegRatioLabelOnly)\n",
    "            , [CropForegroundObject](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropforegroundobject#ai4med.components.transforms.crop_foreground_object.CropForegroundObject)\n",
    "            , [CropSubVolumeCenter](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropsubvolumecenter#ai4med.components.transforms.crop_sub_volume_center.CropSubVolumeCenter)\n",
    "            , [CropRandomSizeWithDisplacement](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=croprandomsizewithdisplacement#ai4med.components.transforms.crop_random_size_w_displacement.CropRandomSizeWithDisplacement)\n",
    "            , [CropFixedSizeRandomCenter](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropfixedsizerandomcenter#ai4med.components.transforms.crop_fixed_size_random_center.CropFixedSizeRandomCenter)\n",
    "        4. Deformable transformations\n",
    "            [FastPosNegRatioCropROI](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=fastposnegratiocroproi#ai4med.components.transforms.fast_pos_neg_ratio_crop_roi.FastPosNegRatioCropROI)\n",
    "        5. Intensity Transforms\n",
    "            [ScaleIntensityRange](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scaleintensityrange#ai4med.components.transforms.scale_intensity_range.ScaleIntensityRange)\n",
    "            , [ScaleIntensityOscillation](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scaleintensityoscillation#ai4med.components.transforms.scale_intensity_oscillation.ScaleIntensityOscillation)\n",
    "            , [AddGaussianNoise](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=addgaussiannoise#ai4med.components.transforms.add_gaussian_noise.AddGaussianNoise)\n",
    "            , [NormalizeNonzeroIntensities](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=normalizenonzerointensities#ai4med.components.transforms.normalize_nonzero_intensities.NormalizeNonzeroIntensities)\n",
    "            , [CenterData](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=centerdata#ai4med.components.transforms.center_data.CenterData)\n",
    "            , [AdjustContrast](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=adjustcontrast#ai4med.components.transforms.adjust_contrast.AdjustContrast)\n",
    "            , [RandomGaussianSmooth]()\n",
    "            , [RandomMRBiasField]()\n",
    "        6. Augmentation Transforms\n",
    "            [RandomZoom](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomzoom#ai4med.components.transforms.random_zoom.RandomZoom)\n",
    "            , [RandomAxisFlip](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomaxisflip#ai4med.components.transforms.random_axis_flip.RandomAxisFlip)\n",
    "            , [RandomSpatialFlip](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomspatialflip#ai4med.components.transforms.random_spatial_flip.RandomSpatialFlip)\n",
    "            , [RandomRotate2D](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomrotate2d#ai4med.components.transforms.random_rotate_2d.RandomRotate2D)\n",
    "            , [RandomRotate3D](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomrotate3d#ai4med.components.transforms.random_rotate_3d.RandomRotate3D)\n",
    "        7. Special transforms \n",
    "            [AddExtremePointsChannel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=addextremepointschannel#ai4med.components.transforms.add_extreme_points_channel.AddExtremePointsChannel)\n",
    "            , [SplitAcrossChannels](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=splitacrosschannels#ai4med.components.transforms.split_across_channels.SplitAcrossChannels)\n",
    "            , [SplitBasedOnLabel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=splitbasedonlabel#ai4med.components.transforms.split_based_on_label.SplitBasedOnLabel)\n",
    "            , [ThresholdValues](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=thresholdvalues#ai4med.components.transforms.apply_threshold.ThresholdValues)\n",
    "            , [SplitBasedOnBratsClasses](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=splitbasedonbratsclasses#ai4med.components.transforms.split_based_on_brats_classes.SplitBasedOnBratsClasses)\n",
    "            , [ConvertToMultiChannelBasedOnLabel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=converttomultichannelbasedonlabel#ai4med.components.transforms.convert_to_multi_channel_based_on_label.ConvertToMultiChannelBasedOnLabel)\n",
    "            , [KeepLargestCC](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=keeplargestcc#ai4med.components.transforms.keep_largest_connected_component.KeepLargestCC)\n",
    "            , [CopyProperties](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=copyproperties#ai4med.components.transforms.copy_properties.CopyProperties)\n",
    "            , [ConvertToMultiChannelBasedOnBratsClasses](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=converttomultichannelbasedonbratsclasses#ai4med.components.transforms.convert_to_multi_channel_based_on_brats_classes.ConvertToMultiChannelBasedOnBratsClasses)\n",
    "            , [ArgmaxAcrossChannels](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=argmaxacrosschannels#ai4med.components.transforms.argmax_across_channels.ArgmaxAcrossChannels)       \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confFile=MMAR_ROOT+\"/config/trn_base.json\"\n",
    "printFile(confFile,9,8)\n",
    "printFile(confFile,16,8)\n",
    "printFile(confFile,25,20)\n",
    "printFile(confFile,108,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "\n",
    "3. Validation config which includes:\n",
    "    1. Metric \n",
    "    2. pre-transforms. Since these transforms are usually a subset from the pre-transforms in the training section, \n",
    "    we can use the alias to point to these transforms by name as ` \"ref\": \"LoadNifti\"`. \n",
    "    In case we use 2 transforms with the same name as `ScaleByResolution` \n",
    "    we can give each an alias to refer to as `\"name\": \"ScaleByResolution#ScaleImg\"` \n",
    "    then refer to it in the validation section as `ScaleImg` \n",
    "    3. Image pipeline\n",
    "    4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confFile=MMAR_ROOT+\"/config/trn_base.json\"\n",
    "printFile(confFile,120,13)\n",
    "printFile(confFile,135,16)\n",
    "printFile(confFile,152,12)\n",
    "printFile(confFile,164,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "## Start TensorBoard \n",
    "Before launching a training run or while the neural network is training, \n",
    "users can monitor the accuracy and other metrics using tensorboard in a side jupyter lab tab as shown below\n",
    " <br>![tb](screenShots/TensorBoard.png)<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lets start training\n",
    "Now that we have our training configuration, to start training simply run `train.sh` as below. \n",
    "Please keep in mind that we have setup a dummy dataset with one file to train a small neural network quickly (we only train for 2 epochs). \n",
    "Please see exercises on how to easily switch data and train a real segmentation network.\n",
    "\n",
    "**_Note:_** We have renamed `train.sh` to `train_W_Config.sh` as we modified it to accept parameters with the configuration to use       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! $MMAR_ROOT/commands/train_W_Config.sh trn_base.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Now let us navigate the `models` directory, which would includes out models and the tensorboard files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls -la $MMAR_ROOT/models/trn_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "\n",
    "## Export Model\n",
    "\n",
    "To export the model we simply run `export.sh` which will: \n",
    "- Remove back propagation information from checkpoint files\n",
    "- Generate two frozen graphs in the models folder\n",
    "This optimized model will be used by Triton Inference server in the Clara Deploy SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! $MMAR_ROOT/commands/export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "\n",
    "lets check out what was created in the folder. \n",
    "after running cell below you should see:\n",
    "1. Frozen File Generated: /claraDevDay/MMARs/GettingStarted/commands/../models/trn_base/model.fzn.pb\n",
    "2. TRT File Generated: /claraDevDay/MMARs/GettingStarted/commands/../models/trn_base/model.trt.pb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!ls -la $MMAR_ROOT/models/trn_base/*.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "## Evaluate and Prediction \n",
    "Now that we have trained our model we would like to run evaluation to get some statistics and also do inference to see the resulting prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 1. Evaluate \n",
    "To run evaluation on your validation dataset you should run `validate.sh`. \n",
    "This will run evaluation on the validation dataset and place it in the `MMAR_EVAL_OUTPUT_PATH` as configured in the [environment.json](config/environment.json) \n",
    "file (default is eval folder). \n",
    "This evaluation would give min, max, mean of the metric as specified in the config_validation file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! $MMAR_ROOT/commands/validate.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Now let us see the results in the folder by running cells below. \n",
    "You should see statistics and dice per file in the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls -la $MMAR_ROOT/eval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# statistic summary\n",
    "!cat $MMAR_ROOT/eval/mean_dice_class1_summary_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!cat $MMAR_ROOT/eval/mean_dice_class1_raw_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 2. Predict\n",
    "\n",
    "To run inference on validation dataset or test dataset you should run `infer.sh`. \n",
    "This will run prediction on the validation dataset and place it in the `MMAR_EVAL_OUTPUT_PATH` as configured in the \n",
    "[environment.json](config/environment.json) file (default is eval folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! $MMAR_ROOT/commands/infer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Now lets see results in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls -la $MMAR_ROOT/eval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls -la $MMAR_ROOT/eval/spleen_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Multi-GPU Training\n",
    "Clara train aims to simplify scaling and the utilization of all available gpu resources. \n",
    "Using the same config we already used for train we can simply invoke `train_2gpu.sh` to train on multiple gpus. \n",
    "We use MPI and Horovod to speed up training and passing weights between GPUs as shown below\n",
    "<br>![tb](screenShots/MultiGPU.png)<br> \n",
    "\n",
    "Let us examine the `train_2gpu.sh` script by running cell below. \n",
    "You can see we are changing the learning rate as the batch size has doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "printFile(MMAR_ROOT+\"/commands/train_2gpu.sh\",0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Lets give it a try and run cell below to train on 2 gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! $MMAR_ROOT/commands/train_2gpu.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "\n",
    "# Exercise:\n",
    "Now that you are familiar with the Clara Train SDK, you can try to: \n",
    "1. Train on a full spleen dataset; to do this you could:\n",
    "    1. Download the spleen dataset using the [download](download) Notebook\n",
    "    2. Switch the dataset file in the [environment.json](config/environment.json)\n",
    "    3. rerun `train.sh`\n",
    "2. Explore different model architectures, losses, transformations by modifying or creating a new config file and running training\n",
    "3. Experiment with multi-GPU training by changing the number of gpus to train on from 2 to 3 or 4. \n",
    "You can edit [train_2gpu.sh](commands/train_2gpu.sh) then rerun the script.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
