{"cells":[{"metadata":{"id":"RjHxX2RdL6zZ","outputId":"c7ac4b7e-d170-4855-a8ae-d571b64574cd","trusted":true},"cell_type":"code","source":"!pip install meditorch\n!pip install torchsummary","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting meditorch\n  Downloading meditorch-0.0.3-py3-none-any.whl (16 kB)\nRequirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.6/site-packages (from meditorch) (1.4.0)\nRequirement already satisfied: numpy>=1.14.1 in /opt/conda/lib/python3.6/site-packages (from meditorch) (1.18.1)\nRequirement already satisfied: scikit-image>=0.15.0 in /opt/conda/lib/python3.6/site-packages (from meditorch) (0.16.2)\nCollecting patool>=1.2\n  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n\u001b[K     |████████████████████████████████| 77 kB 2.6 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.15.0->meditorch) (1.4.1)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.15.0->meditorch) (2.6.1)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.15.0->meditorch) (3.0.3)\nRequirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.15.0->meditorch) (5.4.1)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.15.0->meditorch) (2.4)\nRequirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.15.0->meditorch) (1.1.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->meditorch) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->meditorch) (1.1.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->meditorch) (2.4.6)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->meditorch) (2.8.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.15.0->meditorch) (4.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->meditorch) (1.14.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->meditorch) (45.2.0.post20200210)\nInstalling collected packages: patool, meditorch\nSuccessfully installed meditorch-0.0.3 patool-1.12\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","name":"stdout"}]},{"metadata":{"id":"KLbWwuezxQI9"},"cell_type":"markdown","source":"# **EDD**"},{"metadata":{"id":"e_fRixUtyEQF"},"cell_type":"markdown","source":"**1.Downloading data and extracting files to ./EDD2020**  \nACCEPT COMPETITION RULES TO ACCESS DATA at https://edd2020.grand-challenge.org/\n"},{"metadata":{"id":"MoNTIUi-_sxD","outputId":"20b56c53-833c-49f5-b185-8fa3935bc9d5","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Jan 16 02:21:58 2020\n\n@author: shariba\n\"\"\"\n#!pip install boto3 (run this in your terminal/ ipython console)\n#s3://ieee-dataport/competition/6486/EDD2020_release-I_2020-01-15_v2.zip\n\nimport boto3\ns3 = boto3.client('s3')\n\n#==========Communicate licence for dataset==========\nprint('Data you are downloading is protected under: Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)')\nprint('see: https://creativecommons.org/licenses/by-nc/4.0/')\n#===================================================\nprint('downloading... wait!!!')\ns3 = boto3.client('s3', aws_access_key_id='AKIAIKFHV2ZSST7NLSEA' , aws_secret_access_key='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')\ns3.download_file('ieee-dataport', 'competition/6486/EDD2020_release-I_2020-01-15_v2.zip', 'EDD2020.zip')\nprint('done... good luck with the EDD2020 challenge!!!')\n#===================================================\nimport zipfile\nwith zipfile.ZipFile('EDD2020.zip', 'r') as zip_ref:\n    \n    zip_ref.extractall('EDD2020')\n!ls ./EDD2020/EDD2020_release-I_2020-01-15","execution_count":2,"outputs":[{"output_type":"stream","text":"Data you are downloading is protected under: Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\nsee: https://creativecommons.org/licenses/by-nc/4.0/\ndownloading... wait!!!\ndone... good luck with the EDD2020 challenge!!!\nEDD_readme.md\tbbox\t\tmasks\t       originalImages\nEDD_readme.pdf\tclass_list.txt\tmasksPerClass\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import cv2\n# import numpy as np\n# from matplotlib import pyplot as plt\n# %matplotlib inline\n# #opencv reads images as h(y-axis),w(x-axis),channels\n# img = cv2.imread('../input/abominal/yeti_large.jpg')[:,:,::-1]\n# img = flip_img_vertical(img)\n# plt.imshow(img)\n# plt.show()\n# np.asarray(img).shape","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize_my_images(src,dst,is_masks):\n    import cv2\n    import os\n    import numpy as np\n\n    i = 0\n    img_size = 224\n    path = src\n    for img_name in sorted(os.listdir(path)):\n        img = None\n#         print(img_name)\n        try:\n            if not is_masks:\n                img = cv2.imread(os.path.join(path, img_name))\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            elif is_masks:\n                img = cv2.imread(os.path.join(path, img_name),cv2.IMREAD_GRAYSCALE)\n    \n            h, w = img.shape[:2]\n            a1 = w/h\n            a2 = h/w\n\n            if(a1 > a2):\n                #print('a1 > a2')\n                # if width greater than height\n                r_img = cv2.resize(img, (round(img_size * a1), img_size), interpolation = cv2.INTER_AREA)\n                margin = int(r_img.shape[1]/6)\n                crop_img = r_img[0:img_size, margin:(margin+img_size)]\n\n            elif(a1 < a2):\n                #print('a1 < a2')\n                # if height greater than width\n                r_img = cv2.resize(img, (img_size, round(img_size * a2)), interpolation = cv2.INTER_AREA)\n                margin = int(r_img.shape[0]/6)\n                crop_img = r_img[margin:(margin+img_size), 0:img_size]\n\n            elif(a1 == a2):\n                #print('a1== a2')\n                # if height and width are equal\n                r_img = cv2.resize(img, (img_size, round(img_size * a2)), interpolation = cv2.INTER_AREA)\n                crop_img = r_img[0:img_size, 0:img_size]\n\n            if(crop_img.shape[0] != img_size or crop_img.shape[1] != img_size):\n                #print('someting....')\n                crop_img = r_img[0:img_size, 0:img_size]\n\n            if(crop_img.shape[0] == img_size and crop_img.shape[1] == img_size):\n                if not is_masks:\n                    cv2.imwrite(dst + img_name, crop_img)#SAVING AS RGB FORMAT \n                    #display_image(crop_img,is_mask=False)\n                elif is_masks:\n                    #converting masks: tif--->jpg   \n                    cv2.imwrite(dst + img_name.split('.')[0]+'.jpg', crop_img)\n                    #display_image(crop_img,is_mask=True)\n                #print(\"Saving \"+ img_name.split('.')[0] + \" with dims: \" + str(crop_img.shape))\n                i += 1\n            #print('><'*20)\n        except:\n            print('Could not save image.')\n    if is_masks:\n        print('no of masks saved: ',i)\n    else:\n        print('no of images saved: ',i)\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf ./EDD2020/resized_masks/\n!mkdir ./EDD2020/resized_masks/\nresize_my_images('./EDD2020/EDD2020_release-I_2020-01-15/masks/','./EDD2020/resized_masks/',is_masks=True)","execution_count":5,"outputs":[{"output_type":"stream","text":"no of masks saved:  502\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf ./EDD2020/resized_images/\n!mkdir ./EDD2020/resized_images/\nresize_my_images('./EDD2020/EDD2020_release-I_2020-01-15/originalImages/','./EDD2020/resized_images/',is_masks=False)","execution_count":6,"outputs":[{"output_type":"stream","text":"no of images saved:  386\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Note: no of masks = 502, no of images=386.   \nSome images have more than 1 conditions-- BE,suspicious,HGD,cancer,polyp"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO DISPLAY ANY IMAGE(RGB/GREY)\nimport cv2\ndef display_image(img,is_mask=False):\n    from matplotlib import pyplot as plt\n    %matplotlib inline\n    if is_mask:\n        plt.imshow(img,cmap='gray')\n    else:\n        plt.imshow(img)\n    plt.show()\n# mask = cv2.imread('./EDD2020/resized_masks/EDD2020_ACB0000_cancer.jpg',cv2.IMREAD_GRAYSCALE)\n# display_image(mask,True)","execution_count":7,"outputs":[]},{"metadata":{"id":"AC4VNDt4vggK"},"cell_type":"markdown","source":"**2.Defining class for preparing the dataset and other utility functions**\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def flip_img_horizontal(img):\n    '''\n    assuming RGB image is in (h,w,c) format\n    '''\n    return img[:,::-1,:]\ndef flip_img_vertical(img):\n    '''\n    assuming RGB image is in (h,w,c) format\n    '''\n    return img[::-1,:,:]\ndef flip_mask_horizontal(mask):\n    '''\n    assuming greyscale mask is in (h,w) format\n    '''\n    return mask[:,::-1]\ndef flip_mask_vertical(mask):\n    '''\n    assuming greyscale mask is in (h,w) format\n    '''\n    return mask[::-1,:]","execution_count":8,"outputs":[]},{"metadata":{"id":"O7O_z0kiyAST","trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\nimport torch\nimport os\nimport numpy as np\nimport glob\nfrom PIL import Image\nfrom skimage.transform import resize\nfrom tqdm.notebook import tqdm\ndef load_image(path,is_mask):\n    if not is_mask:\n        return np.asarray(Image.open(path).convert(\"RGB\"))\n    else:\n        return np.asarray(Image.open(path).convert('L'))\n\ndef load_set(folder,is_mask,shuffle=False):\n    data = []\n    img_list = sorted(glob.glob(os.path.join(folder, '*.jpg')))\n    if shuffle:\n        np.random.shuffle(img_list)\n    for img_fn in img_list:\n        img = load_image(img_fn,is_mask)\n        data.append(img)\n    return data, img_list\n\nclass EDD(Dataset):\n    '''\n    Class for preparing the EDD2020 dataset\n    '''\n    def __init__(self, root, img_transform=None):\n        self.root = root\n        self.img_transform = img_transform\n        self.original_images = None\n        self.masks = None\n        self.labels = None\n        self._extract_images_and_segments(root)\n\n    def __getitem__(self,index):\n        img = self.original_images[index]\n        mask = self.masks[index]\n#         label = self.labels[index]\n#         label = torch.as_tensor(label, dtype=torch.int32)\n    \n        if self.img_transform:\n            img = self.img_transform(img)\n            \n        else:\n            transform_to_tensor = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225],)\n                #Normalizing makes images appear weird but necessary for resnet\n            ])\n            img = transform_to_tensor(img)\n        \n        return img, mask\n\n    def __len__(self):\n        return len(self.original_images)\n    \n    def _extract_images_and_segments(self,global_path):\n        '''\n        Function to process images and their respective masks.\n        It sets  self.original_images and self.masks to processed images at the end.\n        '''\n        images_path = os.path.join(global_path, 'resized_images')\n        all_images, img_filenames = load_set(folder=images_path,is_mask=False)\n        img_filenames_with_ext = [os.path.split(fn)[-1] for fn in img_filenames]\n        img_filenames_wo_ext = [fn[:fn.rfind('.')] for fn in img_filenames_with_ext]\n\n        classes = ['BE','suspicious','HGD','cancer','polyp']\n\n        masks_path = os.path.join(global_path, 'resized_masks')\n        all_masks, mask_filenames = load_set(folder=masks_path,is_mask=True)\n        mask_filenames_with_ext = [os.path.split(fn)[-1] for fn in mask_filenames]\n        mask_filenames_wo_ext = [fn[:fn.rfind('.')] for fn in mask_filenames_with_ext]\n        temp_dict={}#contains 502 mask filenames as keys and respective masks as values\n        for i in range(len(all_masks)):\n            temp_dict[mask_filenames_wo_ext[i]]=all_masks[i]\n\n        all_masks=[]\n        all_labels=[]\n        for img in img_filenames_wo_ext:\n            masks_for_img = []\n            temp_labels = []\n            for c in classes:\n                try:\n                    mask_file_name = img+'_'+c\n                    temp_dict[mask_file_name] = np.where(temp_dict[mask_file_name] > 0, 1, 0)\n                    temp_dict[mask_file_name] = temp_dict[mask_file_name].astype(np.float32)\n                    masks_for_img.append(temp_dict[mask_file_name].reshape(temp_dict[mask_file_name].shape+ (1,)))\n                    temp_labels.append(1)\n                except KeyError:\n                    dummy = np.zeros((224, 224)).astype(np.float32)\n                    masks_for_img.append(dummy.reshape(dummy.shape + (1,)))\n                    temp_labels.append(0)\n            temp = None\n            temp = np.concatenate(masks_for_img,2)#temp.shape (224, 224, 5)\n            temp = temp.reshape((1,)+temp.shape)#temp.shape (1, 224, 224, 5)\n            all_masks.append(temp)\n            all_labels.append(temp_labels)\n            \n        all_masks = np.vstack(all_masks)#all_masks.shape (386, 224, 224, 5)\n        all_masks = np.moveaxis(all_masks,source=3,destination=1)#all_masks.shape (386, 5, 224, 224)\n        \n        all_images = np.asarray(all_images)\n        all_images = all_images.astype(np.uint8)\n        augmented_images = []\n        augmented_masks = []\n        print(\">>>>>>>>>>>>>>>>Augmenting to increase data size<<<<<<<<<<<<<<<<<<<<<<<<<\")\n        for image,masks in tqdm(zip(all_images,all_masks)):\n            image = np.asarray(image)\n            #print(image.shape,masks.shape)\n            hflip_img = flip_img_horizontal(image)\n            vflip_img = flip_img_vertical(image)\n            temp_hor_masks = []#to store horizontally flipped masks for an image,which are later np.concatenated\n            temp_ver_masks = []#to store vertically flipped masks for an image,which are later np.concatenated\n            for mask in masks:\n                temp_h = flip_mask_horizontal(mask).reshape((1,)+mask.shape)#reshaping (224,224)-->(1,224,224)\n                temp_hor_masks.append(temp_h)\n                temp_v = flip_mask_vertical(mask).reshape((1,)+mask.shape)#reshaping (224,224)-->(1,224,224)\n                temp_ver_masks.append(temp_v)\n            hflip_masks = np.concatenate(temp_hor_masks,0)#concatenating all 5 masks into 1 (224,224,5)\n            vflip_masks = np.concatenate(temp_ver_masks,0)#concatenating all 5 masks into 1 (224,224,5)\n            #print(\"flipped:\",hflip_img.shape,hflip_masks.shape,vflip_img.shape,vflip_masks.shape)\n            augmented_images.append(hflip_img)\n            augmented_images.append(vflip_img)\n            augmented_masks.append(hflip_masks.reshape((1,)+hflip_masks.shape))\n            augmented_masks.append(vflip_masks.reshape((1,)+vflip_masks.shape))\n            \n        #images\n        augmented_images = np.asarray(augmented_images)\n        augmented_images = augmented_images.astype(np.uint8)\n        print('orignial_images.shape',all_images.shape,'augmented_images.shape',augmented_images.shape)\n        all_images = np.concatenate((all_images,augmented_images),axis=0)\n        print('After augmentation, all_images.shape',all_images.shape)\n        print(\".\"*100)\n        #masks\n        augmented_masks = np.vstack(augmented_masks)\n        print('orignial_masks.shape',all_masks.shape,'augmented_masks.shape',augmented_masks.shape)\n        all_masks = np.concatenate((all_masks,augmented_masks),axis=0)\n        print('After augmentation, all_masks.shape',all_masks.shape)\n        print(\"*\"*150)\n        ################################results display\n        print('len(all_images):',len(all_images),'len(all_masks):',len(all_masks))\n        print('>>>>>>>>>>>Images<<<<<<<<<<<')\n        print('type(all_images):',type(all_images),' all_images.shape:',all_images.shape)\n        print('type(all_images[1]):',type(all_images[1]),' all_images[1].shape:',all_images[1].shape)\n        print('.'*100)\n        print('>>>>>>>>>>>Masks<<<<<<<<<<<<')\n        print('type(all_masks):',type(all_masks),'all_masks.shape:',all_masks.shape)\n        print('type(all_masks[1]):',type(all_masks[1]),'all_masks[1].shape:',all_masks[1].shape)\n        print('.'*100)\n        \n        self.masks = all_masks\n        self.original_images = all_images\n        self.labels = all_labels","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>SCRATCH PAD<<<<<<<<<<<<<<<<<<<<<<<<<\n# from torch.utils.data import DataLoader\n# from torch.utils.data.sampler import SubsetRandomSampler\n# np.random.seed(42)\n# dataset = EDD('./EDD2020/')\n# # for i in range(25):\n# #     print(i+1,':->',type(dataset[i][0]),dataset[i][0].shape,type(dataset[i][1]),dataset[i][1].shape)\n# dataset_size = 100#len(dataset)\n# indices = list(range(dataset_size))\n# split = int(np.floor(0.25 * dataset_size))\n# #np.random.shuffle(indices)\n# train_indices, val_indices = indices[split:], indices[:split]\n# train_sampler = torch.utils.data.sampler.SequentialSampler(train_indices)#change to SubsetRandomSampler\n# loader = DataLoader(dataset, batch_size=4, sampler=train_sampler)","execution_count":19,"outputs":[]},{"metadata":{"id":"Y41kbTZXvuai"},"cell_type":"markdown","source":"**3.instantiating class and splitting it into train,val set**\n"},{"metadata":{"id":"JEsu1HcnmlNA","outputId":"9ae8c54d-4e27-46dd-9657-0d0c133d1618","trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nnp.random.seed(42)\ndef get_edd_loader(path,validation_split=.25,shuffle_dataset=True):\n    dataset = EDD(path) #instantiating the data set.\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    split = int(np.floor(validation_split * dataset_size))\n    if shuffle_dataset :\n        np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n    train_sampler = SubsetRandomSampler(train_indices)\n    valid_sampler = SubsetRandomSampler(val_indices)\n    loader={\n        'train':DataLoader(dataset, batch_size=8, sampler=train_sampler),\n        'val':DataLoader(dataset, batch_size = 4,sampler=valid_sampler)\n    }\n    return loader","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.Defining Trainer and losses**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nimport copy\nimport time\nimport torch\nfrom meditorch.utils.plot import metrics_line, normalise_mask\nimport numpy as np\n\n\n\ndef cal_dice_loss(pred, target, smooth = 1.):\n    pred = pred.contiguous()\n    target = target.contiguous()\n    intersection = (pred * target).sum(dim=2).sum(dim=2)\n    dice_coeff = (2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)\n    dice_loss = 1 - dice_coeff\n    return dice_loss.mean()\n\ndef intersection_over_union(target, pred):\n    target = target.flatten()\n    pred = pred.flatten()\n    intersect = np.dot(target, pred)\n    union = (target + pred).sum() - intersect\n    return intersect/union\n\ndef calc_loss(pred, target, metrics, bce_weight=0.6):\n    bce = torch.nn.functional.binary_cross_entropy_with_logits(pred, target)\n    pred = torch.sigmoid(pred)\n    dice_loss = cal_dice_loss(pred, target)\n    pred_binary = normalise_mask(pred.detach().cpu().numpy())\n    iou = intersection_over_union(target.detach().cpu().numpy(), pred_binary)\n\n    loss = bce * bce_weight + dice_loss * (1 - bce_weight)\n     \n    metrics['bce_loss'] += bce.data.cpu().numpy() * target.size(0)\n    metrics['dice_loss'] += dice_loss.data.cpu().numpy() * target.size(0)\n    metrics['iou'] += iou * target.size(0)\n    metrics['total_loss'] += loss.data.cpu().numpy() * target.size(0)\n    return loss\n\ndef compute_metrics(metrics, epoch_samples):\n    computed_metrics = {}\n    for k in metrics.keys():\n        computed_metrics[k] = metrics[k] / epoch_samples\n    return computed_metrics\n\ndef print_metrics(computed_metrics, phase):\n    outputs = []\n    for k in computed_metrics.keys():\n        outputs.append(\"{}:{:4f}\".format(k, computed_metrics[k]))\n    print(\"\\t{}  -> {}\".format(phase.ljust(5), \" | \".join(outputs)))\n\nclass Trainer(object):\n\n    def __init__(self, model, optimizer=None, scheduler=None):\n\n        super().__init__()\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(\"Customized trainer\\t\",'Device====>',self.device)\n        self.model = model.to(self.device)\n\n        self.optimizer = optimizer\n        if self.optimizer == None:\n            print(\"optimizer undefined, hence using default Adam optimizer..\")\n            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n\n        self.scheduler = scheduler\n        if self.scheduler == None:\n            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.1)\n\n\n    def train_model(self, dataloaders, num_epochs=25):\n        best_model_wts = copy.deepcopy(self.model.state_dict())\n        best_loss = 1e10\n        epochs_metrics = {\n            'train': [],\n            'val': []\n        }\n\n        for epoch in range(num_epochs):\n            print('Epoch {}/{}:'.format(epoch+1, num_epochs))\n\n            since = time.time()\n\n            # Each epoch has a training phase, followed by a validation phase\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    for param_group in self.optimizer.param_groups:\n                        print(\"\\tlearning rate: {:.2e}\".format(param_group['lr']))\n                    self.model.train()  # Set model to training mode\n                else:\n                    self.model.eval()   # Set model to evaluate mode\n                metrics = defaultdict(float)\n                epoch_samples = 0\n\n                for inputs, targets in dataloaders[phase]:#phase is either train or val\n                    masks = targets\n                    #print('>>>>>>>>>>>',inputs.shape,masks.shape,'<<<<<<<<<<<<<<<<<<')\n                \n\n                    inputs = inputs.to(self.device)\n                    masks = masks.to(self.device)\n\n                    # zero the parameter gradients\n                    self.optimizer.zero_grad()\n\n                    # forwar track history for training phase only\n                    with torch.set_grad_enabled(phase == 'train'):\n                        outputs = self.model(inputs)\n                        loss = calc_loss(outputs, masks, metrics)\n\n                        # backward + optimize only if in training phase\n                        if phase == 'train':\n                            loss.backward()\n                            self.optimizer.step()\n\n                    # statistics\n                    epoch_samples += inputs.size(0)\n\n                computed_metrics = compute_metrics(metrics, epoch_samples)\n                print_metrics(computed_metrics, phase)\n                epochs_metrics[phase].append(computed_metrics)\n                epoch_loss = metrics['total_loss'] / epoch_samples\n\n                if phase == 'train':\n                    self.scheduler.step()\n\n                # deep copy the model\n                if phase == 'val' and epoch_loss < best_loss:\n                    print(\"\\tCurrent epoch loss {:4f} is less than previous epoch loss {:4f}\".format(epoch_loss, best_loss))\n                    best_loss = epoch_loss\n                    best_model_wts = copy.deepcopy(self.model.state_dict())\n\n            time_elapsed = time.time() - since\n            print('\\t{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n            print('-' * 10)\n\n        print('Best val loss: {:4f}'.format(best_loss))\n        print('Saving best model')\n        self.model.load_state_dict(best_model_wts)# load best model weights\n\n        metrics_line(epochs_metrics)\n\n    def predict(self, X, threshold=0.5):\n        self.model.eval()\n        inputs = X.to(self.device)\n        pred = self.model(inputs)\n        pred = pred.data.cpu().numpy()\n        pred = normalise_mask(pred, threshold)\n        return pred\n\n\n","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5.Defining the model: Unet+Resnet**  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\ndef convrelu(in_channels, out_channels, kernel, padding):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n        nn.ReLU(inplace=True),\n    )\n\n\nclass UNetResNet(nn.Module):\n\n    def __init__(self, in_channel, n_classes):\n        super().__init__()\n        self.base_model = models.resnet50()\n        self.base_model.load_state_dict(torch.load(\"../input/resnet50/resnet50.pth\"))\n\n        self.base_layers = list(self.base_model.children())\n\n        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n\n        self.conv_original_size0 = convrelu(in_channel, 64, 3, 1)\n        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n\n        self.conv_last = nn.Conv2d(64, n_classes, 1)\n\n    def forward(self, input):\n        x_original = self.conv_original_size0(input)\n        x_original = self.conv_original_size1(x_original)\n\n        layer0 = self.layer0(input)\n        layer1 = self.layer1(layer0)\n        layer2 = self.layer2(layer1)\n        layer3 = self.layer3(layer2)\n        layer4 = self.layer4(layer3)\n\n        layer4 = self.layer4_1x1(layer4)\n        x = self.upsample(layer4)\n        layer3 = self.layer3_1x1(layer3)\n        x = torch.cat([x, layer3], dim=1)\n        x = self.conv_up3(x)\n\n        x = self.upsample(x)\n        layer2 = self.layer2_1x1(layer2)\n        x = torch.cat([x, layer2], dim=1)\n        x = self.conv_up2(x)\n\n        x = self.upsample(x)\n        layer1 = self.layer1_1x1(layer1)\n        x = torch.cat([x, layer1], dim=1)\n        x = self.conv_up1(x)\n\n        x = self.upsample(x)\n        layer0 = self.layer0_1x1(layer0)\n        x = torch.cat([x, layer0], dim=1)\n        x = self.conv_up0(x)\n\n        x = self.upsample(x)\n        x = torch.cat([x, x_original], dim=1)\n        x = self.conv_original_size2(x)\n\n        out = self.conv_last(x)\n\n        return out","execution_count":40,"outputs":[]},{"metadata":{"id":"DuQTnW__nrUr","trusted":true},"cell_type":"code","source":"#from meditorch.nn.models import UNetResNet\nfrom torchsummary import summary\nimport torch.optim as optim\n#from meditorch.nn import Trainer\n\nfrom torch.optim import lr_scheduler\nfrom meditorch.utils.plot import plot_image_truemask_predictedmask\n\nmodel = UNetResNet(in_channel=3,n_classes=5)\noptimizer_func = optim.Adam(model.parameters(), lr=1e-3)\n#optimizer_func = torch.optim.SGD(model.parameters(), weight_decay=1e-4, lr = 0.001, momentum=0.9)\nscheduler = lr_scheduler.StepLR(optimizer_func, step_size=5, gamma=0.01)\ntrainer = Trainer(model, optimizer=optimizer_func, scheduler=scheduler)","execution_count":41,"outputs":[{"output_type":"stream","text":"Customized trainer\t Device====> cuda\n","name":"stdout"}]},{"metadata":{"id":"gcTb5_suwQ4M"},"cell_type":"markdown","source":"**6.Training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"loader = get_edd_loader('./EDD2020/',validation_split=.20,shuffle_dataset=True)","execution_count":42,"outputs":[{"output_type":"stream","text":">>>>>>>>>>>>>>>>Augmenting to increase data size<<<<<<<<<<<<<<<<<<<<<<<<<\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c8d6e09ecf4ed59e8b9d79f5247b90"}},"metadata":{}},{"output_type":"stream","text":"\norignial_images.shape (386, 224, 224, 3) augmented_images.shape (772, 224, 224, 3)\nAfter augmentation, all_images.shape (1158, 224, 224, 3)\n....................................................................................................\norignial_masks.shape (386, 5, 224, 224) augmented_masks.shape (772, 5, 224, 224)\nAfter augmentation, all_masks.shape (1158, 5, 224, 224)\n******************************************************************************************************************************************************\nlen(all_images): 1158 len(all_masks): 1158\n>>>>>>>>>>>Images<<<<<<<<<<<\ntype(all_images): <class 'numpy.ndarray'>  all_images.shape: (1158, 224, 224, 3)\ntype(all_images[1]): <class 'numpy.ndarray'>  all_images[1].shape: (224, 224, 3)\n....................................................................................................\n>>>>>>>>>>>Masks<<<<<<<<<<<<\ntype(all_masks): <class 'numpy.ndarray'> all_masks.shape: (1158, 5, 224, 224)\ntype(all_masks[1]): <class 'numpy.ndarray'> all_masks[1].shape: (5, 224, 224)\n....................................................................................................\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.train_model(loader, num_epochs=40)","execution_count":27,"outputs":[{"output_type":"stream","text":"Epoch 1/40:\n\tlearning rate: 1.00e-03\n","name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-00007f95363f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-0fcb52d61afb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, dataloaders, num_epochs)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mepoch_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#phase is either train or val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;31m#print('>>>>>>>>>>>',inputs.shape,masks.shape,'<<<<<<<<<<<<<<<<<<')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-e27a4701db09>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"metadata":{},"cell_type":"markdown","source":"#### total_loss = bce_loss . (0.6) + dice_loss . (1 - 0.6)"},{"metadata":{},"cell_type":"markdown","source":"### 7. Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom functools import reduce\n\n\ndef metrics_line(data):\n    phases = list(data.keys())\n    metrics = list(data[phases[0]][0].keys())\n\n    i = 0\n    fig, axs = plt.subplots(1, len(metrics))\n    fig.set_figheight(4)\n    fig.set_figwidth(4*len(metrics))\n    for metric in metrics:\n        for phase in phases:\n            axs[i].plot([i[metric] for i in data[phase]], label=phase)\n        axs[i].set_title(metric)\n        i+=1\n\n    plt.legend()\n    plt.show()\n\n\ndef normalise_mask(mask, threshold=0.5):\n    mask[mask > threshold] = 1\n    mask[mask <= threshold] = 0\n    return mask\n\ndef reverse_transform(inp):\n    inp = inp.numpy().transpose((1, 2, 0))\n    inp = np.clip(inp, 0, 1)\n    inp = (inp * 255).astype(np.uint8)\n    return inp\n\ndef plot_img_array(img_array, ncol=3):\n    nrow = len(img_array) // ncol\n\n    f, plots = plt.subplots(nrow, ncol, sharex='all', sharey='all', figsize=(ncol * 4, nrow * 4))\n    for i in range(len(img_array)):\n        plots[i // ncol, i % ncol]\n        plots[i // ncol, i % ncol].imshow(img_array[i])\n\ndef plot_image_truemask_predictedmask2(images, labels, preds):\n\n    input_images_rgb = [reverse_transform(x) for x in images]\n    target_masks_rgb = [masks_to_coloredmasks(x) for x in labels]\n    pred_masks_rgb   = [masks_to_coloredmasks(x) for x in preds]\n\n    img_arrays = [input_images_rgb, target_masks_rgb, pred_masks_rgb]\n    flatten_list = reduce(lambda x,y: x+y, zip(*img_arrays))\n    plot_img_array(np.array(flatten_list), ncol=len(img_arrays))\n\ndef apply_mask_color(mask, mask_color):\n    colored_mask = np.concatenate(([mask[ ... , np.newaxis] * color for color in mask_color]), axis=2)\n    return colored_mask.astype(np.uint8)\n\ndef masks_to_coloredmasks(mask, normalise=True, colors=None):\n    '''\n    To assign colors to the different classes, for identification purpose only!\n    '''\n    segments_colors = np.asarray([(0, 255, 255), (0,0,255), (0, 255, 0), (255, 0, 0),(0, 0, 0)])\n    if colors is not None:\n        segments_colors = colors\n\n    if normalise:\n        normalise_mask(mask)\n\n    mask_colored = np.concatenate( [ [apply_mask_color(mask[i], segments_colors[i])] for i in range(len(mask)) ] )\n    mask_colored = np.max(mask_colored, axis=0)\n\n    mask_colored = np.where(mask_colored.any(-1,keepdims=True),mask_colored,255)\n\n    return mask_colored\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images, target = next(iter(loader['val']))#fetch the next batch of images and respective masks\npreds = trainer.predict(images)\nprint(images.shape,target.shape,preds.shape)\nplot_image_truemask_predictedmask2(images, target, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other approaches  : \n--under construction.  \nModelling the distribution of each one of the RGB pixels of the image where the masks show positive\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_sus = []\nall_cancer = []\nall_image = []\nfor batch in loader:\n    image = batch[0][0]#torch.Size([3, 224, 224])\n    mask = batch[1][0]\n    be = mask[0]\n    sus = mask[1]\n    hgd = mask[2]\n    cancer = mask[3]\n    polyp = mask[4]\n    all_image.append(image)\n    all_cancer.append(cancer)\n    all_sus.append(sus)\nlen(all_sus),len(all_cancer),len(all_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer_index = []\ncancer_coords= []\nall_cancer = all_sus\nfor i,mask in enumerate(all_cancer):\n    if np.argwhere(np.asarray(mask)>0).shape[0]!=0:\n        cancer_index.append(i)\n        cancer_coords.append(np.argwhere(np.asarray(mask)>0))\nlen(cancer_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_pixels = []\ng_pixels = []\nb_pixels = []\nfor index,coords in zip(cancer_index,cancer_coords):\n    print(i,coords.shape)\n    r=np.asarray(all_image[index][0])\n    g=np.asarray(all_image[index][1])\n    b=np.asarray(all_image[index][2])\n    for coord in coords:\n        r_pixels.append(r[coord[0]][coord[1]])\n        g_pixels.append(g[coord[0]][coord[1]])\n        b_pixels.append(b[coord[0]][coord[1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(r_pixels),len(g_pixels),len(b_pixels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Suspicious_train"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\nnp.random.seed(0)\nrx = sns.distplot(r_pixels)\ngx = sns.distplot(g_pixels)\nbx = sns.distplot(b_pixels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cancer_train"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\nnp.random.seed(0)\nrx = sns.distplot(r_pixels)\ngx = sns.distplot(g_pixels)\nbx = sns.distplot(b_pixels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cancer_val"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\nnp.random.seed(0)\nrx = sns.distplot(r_pixels)\ngx = sns.distplot(g_pixels)\nbx = sns.distplot(b_pixels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"Tabz4ks0tZUQ","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"7NfMS48UdYz4","outputId":"31f805a5-8499-44f9-ead2-9652b4dd2542","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for batch_image,batch_mask in (loader['train']): #every iteration fetches data of size batchsize\n    print('adfaf')\n    print(batch_image.shape,batch_mask.shape)\n    for image in batch_image:\n        im = transforms.ToPILImage()(image).convert(\"RGB\")\n        display(im)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dataset[0][0]),dataset[0][0].shape,type(dataset[0][1]),dataset[0][1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO GENERATE MASKS\nt=dataset[5][1]\nt=t.astype(np.float32)\nfor i in range(5):\n    tx=t[i,:,:]\n    tx=tx.reshape((1,)+tx.shape)\n    tx=torch.from_numpy(tx)\n    im=transforms.ToPILImage()(tx)\n    display(im)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t=dataset[0][0]\nfrom torchvision import transforms\nim = transforms.ToPILImage()(t).convert(\"RGB\")\ndisplay(im)\nprint(im.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO COMBINE ALL MULTIPLE MASKS INTO ONE\n    \nif __name__ == '__main__':\n    import tifffile as tiff\n    from tifffile import imsave\n    import cv2\n    import numpy as np\n    import glob\n    import os\n    imgdir='./EDD2020/EDD2020_release-I_2020-01-15/originalImages/'\n    maskdir='./EDD2020/EDD2020_release-I_2020-01-15/masks/'\n    outputdir='./EDD2020/EDD2020_release-I_2020-01-15/outputdir/'\n    \n    os.makedirs(outputdir, exist_ok=True)\n    \n    categoryList = ['BE', 'suspicious', 'HGD' , 'cancer', 'polyp']\n    \n    ext = ['*.jpg']\n    for filename in sorted(glob.glob(imgdir + '/'+ ext[0], recursive = True)):\n        file=filename.split('/')[-1]\n        fileNameOnly = file.split('.')[0]\n        nClasses_annotated = len(glob.glob1(maskdir,fileNameOnly+\"*.tif\"))\n        fileList = glob.glob1(maskdir,fileNameOnly+\"*.tif\")\n       \n        # read original and make stack of mask images\n        image = cv2.imread(filename)\n        height, width = image.shape[:2]\n        mask = np.zeros([height, width, len(categoryList)], dtype=np.uint8)\n        \n        for i in range (0, nClasses_annotated):\n            f = fileList[i].split('_')[-1]\n            if f.split('.')[0] == 'BE':\n                mask[:,:, 0] = tiff.imread(os.path.join(maskdir,fileList[i]))\n            elif (f.split('.')[0]== 'suspicious'):\n                mask[:,:, 1] = tiff.imread(os.path.join(maskdir,fileList[i]))\n            elif (f.split('.')[0] == 'HGD'):\n                mask[:,:, 2] = tiff.imread(os.path.join(maskdir,fileList[i]))\n            elif (f.split('.')[0] == 'cancer'):\n                mask[:,:, 3] = tiff.imread(os.path.join(maskdir,fileList[i]))\n            elif (f.split('.')[0] == 'polyp'):\n                mask[:,:, 4] = tiff.imread(os.path.join(maskdir,fileList[i]))\n                \n                \n            im_mask = mask.transpose([2,0,1])    \n            imsave(os.path.join(outputdir,fileNameOnly+'_mask.tif'), im_mask)      \n            ","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"EDD2020.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}